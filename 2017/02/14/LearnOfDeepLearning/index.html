<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="深度学习,Keras,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="Tensorflow1.0正式发布，谷歌首届Tensorflow开发者大会在山景召开，深度学习迎来新的高潮和狂欢。随着深度学习框架的普及和推广，会有越来越多人加入到这场盛宴中来，就像Android技术的普及使得开发人员迅速扩大。在这里給大家带来一套小白入门深度学习的基础教程，使用得是Keras，一个高级神经网络库，同时也是Tensorflow1.0引进的一个高层API。">
<meta name="keywords" content="深度学习,Keras">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习从小白到入门 —— 基于keras的深度学习基本概念讲解">
<meta property="og:url" content="http://yoursite.com/2017/02/14/LearnOfDeepLearning/index.html">
<meta property="og:site_name" content="shikanon">
<meta property="og:description" content="Tensorflow1.0正式发布，谷歌首届Tensorflow开发者大会在山景召开，深度学习迎来新的高潮和狂欢。随着深度学习框架的普及和推广，会有越来越多人加入到这场盛宴中来，就像Android技术的普及使得开发人员迅速扩大。在这里給大家带来一套小白入门深度学习的基础教程，使用得是Keras，一个高级神经网络库，同时也是Tensorflow1.0引进的一个高层API。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/img/softmax-formula.png">
<meta property="og:image" content="http://yoursite.com/img/softmax-image.jpg">
<meta property="og:image" content="http://yoursite.com/img/softmax-regression-scalargraph.png">
<meta property="og:image" content="http://yoursite.com/img/cross-entropy.jpeg">
<meta property="og:image" content="http://yoursite.com/img/scg.jpeg">
<meta property="og:image" content="http://yoursite.com/img/LearnOfDeepLearning_12_1.png">
<meta property="og:image" content="http://yoursite.com/img/LearnOfDeepLearning_14_1.png">
<meta property="og:image" content="http://yoursite.com/img/LearnOfDeepLearning_18_0.png">
<meta property="og:image" content="http://yoursite.com/img/ORX_problem.png">
<meta property="og:image" content="http://yoursite.com/img/LearnOfDeepLearning_22_1.png">
<meta property="og:image" content="http://yoursite.com/img/sigmoid-formula.jpg">
<meta property="og:image" content="http://yoursite.com/img/sigmoid-img.jpg">
<meta property="og:image" content="http://yoursite.com/img/relu-img.jpg">
<meta property="og:image" content="http://yoursite.com/img/big-gap.png">
<meta property="og:image" content="http://yoursite.com/img/dropout-1.png">
<meta property="og:updated_time" content="2017-02-17T03:07:03.756Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习从小白到入门 —— 基于keras的深度学习基本概念讲解">
<meta name="twitter:description" content="Tensorflow1.0正式发布，谷歌首届Tensorflow开发者大会在山景召开，深度学习迎来新的高潮和狂欢。随着深度学习框架的普及和推广，会有越来越多人加入到这场盛宴中来，就像Android技术的普及使得开发人员迅速扩大。在这里給大家带来一套小白入门深度学习的基础教程，使用得是Keras，一个高级神经网络库，同时也是Tensorflow1.0引进的一个高层API。">
<meta name="twitter:image" content="http://yoursite.com/img/softmax-formula.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/02/14/LearnOfDeepLearning/">





  <title> 深度学习从小白到入门 —— 基于keras的深度学习基本概念讲解 | shikanon </title>
</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">shikanon</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle">shikanon's blog</p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            Sitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope="" itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/14/LearnOfDeepLearning/">

  <span style="display:none" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="shikanon">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.gif">
  </span>

  <span style="display:none" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="shikanon">
    <span style="display:none" itemprop="logo" itemscope="" itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="shikanon" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                深度学习从小白到入门 —— 基于keras的深度学习基本概念讲解
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-14T10:33:34+08:00">
                2017-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">类别：</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/技术博文/" itemprop="url" rel="index">
                    <span itemprop="name">技术博文</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/技术博文/线性规划/" itemprop="url" rel="index">
                    <span itemprop="name">线性规划</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/02/14/LearnOfDeepLearning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/14/LearnOfDeepLearning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
              <div class="post-description">
                  Tensorflow1.0正式发布，谷歌首届Tensorflow开发者大会在山景召开，深度学习迎来新的高潮和狂欢。随着深度学习框架的普及和推广，会有越来越多人加入到这场盛宴中来，就像Android技术的普及使得开发人员迅速扩大。在这里給大家带来一套小白入门深度学习的基础教程，使用得是Keras，一个高级神经网络库，同时也是Tensorflow1.0引进的一个高层API。
              </div>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><hr>
<h3 id="Author-shikanon"><a href="#Author-shikanon" class="headerlink" title="Author: shikanon"></a>Author: <a href="https://github.com/shikanon" target="_blank" rel="noopener">shikanon</a></h3><h3 id="CreateTime-2017-2-13"><a href="#CreateTime-2017-2-13" class="headerlink" title="CreateTime: 2017/2/13"></a>CreateTime: 2017/2/13</h3><hr>
<ul>
<li><strong>目录</strong><ul>
<li><a href="#1.-softmax">1.softmax</a></li>
<li><a href="#损失函数">2.损失函数</a></li>
<li><a href="#3.-激活函数">3.激活函数</a></li>
<li><a href="#4.-sigmoid">4.sigmoid</a></li>
<li><a href="#5.-ReLu">5.ReLu</a></li>
<li><a href="#6.-学习速率">6.学习速率</a></li>
<li><a href="#7.Dropout">7.Dropout</a></li>
</ul>
</li>
</ul>
<h2 id="一、基础篇"><a href="#一、基础篇" class="headerlink" title="一、基础篇"></a>一、基础篇</h2><p>神经网络中的每个<strong>神经元</strong> 对其所有的输入进行<strong>加权求和</strong>，并添加一个被称为<strong>偏置（bias）</strong> 的常数，然后通过一些<strong>非线性激活函数</strong>来反馈结果。</p>
<p>数据集我们采用深度学习界的<em>Hello-Word</em>———— MNIST手写数字数据集，学习从第一个softmax开始。</p>
<h3 id="1-softmax"><a href="#1-softmax" class="headerlink" title="1. softmax"></a>1. softmax</h3><p>softmax主要用来做多分类问题，是logistic回归模型在多分类问题上的推广，softmax 公式：<br><img src="/img/softmax-formula.png"></p>
<p>当k=2时，转换为逻辑回归形式。</p>
<p>softmax一般作为神经网络最后一层，作为输出层进行多分类，Softmax的输出的每个值都是&gt;=0，并且其总和为1，所以可以认为其为概率分布。</p>
<p><strong>softmax 示意图</strong><br><img src="/img/softmax-image.jpg"></p>
<p><strong>softmax 输出层示意图</strong><br><img src="/img/softmax-regression-scalargraph.png" style="width: 50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%pylab inline</span><br></pre></td></tr></table></figure>
<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Activation, Reshape</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD, Adam</span><br><span class="line"><span class="keyword">from</span> keras.utils.visualize_util <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置随机数种子,保证实验可重复</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#设置线程</span></span><br><span class="line">THREADS_NUM = <span class="number">20</span></span><br><span class="line">tf.ConfigProto(intra_op_parallelism_threads=THREADS_NUM)</span><br><span class="line"></span><br><span class="line">(X_train, Y_train),(X_test, Y_test) = mnist.load_data()</span><br><span class="line">print(<span class="string">'原数据结构：'</span>)</span><br><span class="line">print(X_train.shape, Y_train.shape)</span><br><span class="line">print(X_test.shape, Y_test.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据变换</span></span><br><span class="line"><span class="comment">#分为10个类别</span></span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x_train_1 = X_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line"><span class="comment">#x_train_1 /= 255</span></span><br><span class="line"><span class="comment">#x_train_1 = x_train_1.astype('float32')</span></span><br><span class="line">y_train_1 = np_utils.to_categorical(Y_train, nb_classes)</span><br><span class="line">print(<span class="string">'变换后的数据结构：'</span>)</span><br><span class="line">print(x_train_1.shape, y_train_1.shape)</span><br><span class="line"></span><br><span class="line">x_test_1 = X_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line">y_test_1 = np_utils.to_categorical(Y_test, nb_classes)</span><br><span class="line">print(x_test_1.shape, y_test_1.shape)</span><br></pre></td></tr></table></figure>
<pre><code>原数据结构：
((60000, 28, 28), (60000,))
((10000, 28, 28), (10000,))
变换后的数据结构：
((60000, 784), (60000, 10))
((10000, 784), (10000, 10))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个softmax模型</span></span><br><span class="line"><span class="comment"># neural network with 1 layer of 10 softmax neurons</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># · · · · · · · · · ·       (input data, flattened pixels)       X [batch, 784]        # 784 = 28 * 28</span></span><br><span class="line"><span class="comment"># \x/x\x/x\x/x\x/x\x/    -- fully connected layer (softmax)      W [784, 10]     b[10]</span></span><br><span class="line"><span class="comment">#   · · · · · · · ·                                              Y [batch, 10]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The model is:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Y = softmax( X * W + b)</span></span><br><span class="line"><span class="comment">#              X: matrix for 100 grayscale images of 28x28 pixels, flattened (there are 100 images in a mini-batch)</span></span><br><span class="line"><span class="comment">#              W: weight matrix with 784 lines and 10 columns</span></span><br><span class="line"><span class="comment">#              b: bias vector with 10 dimensions</span></span><br><span class="line"><span class="comment">#              +: add with broadcasting: adds the vector to each line of the matrix (numpy)</span></span><br><span class="line"><span class="comment">#              softmax(matrix) applies softmax on each line</span></span><br><span class="line"><span class="comment">#              softmax(line) applies an exp to each value then divides by the norm of the resulting line</span></span><br><span class="line"><span class="comment">#              Y: output matrix with 100 lines and 10 columns</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(nb_classes, input_shape=(<span class="number">784</span>,)))<span class="comment">#全连接，输入784维度, 输出10维度，需要和输入输出对应</span></span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">sgd = SGD(lr=<span class="number">0.005</span>)</span><br><span class="line"><span class="comment">#binary_crossentropy，就是交叉熵函数</span></span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              optimizer=sgd,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#model 概要</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dense_1 (Dense)                  (None, 10)            7850        dense_input_1[0][0]              
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 10)            0           dense_1[0][0]                    
====================================================================================================
Total params: 7,850
Trainable params: 7,850
Non-trainable params: 0
____________________________________________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Callback, TensorBoard</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建一个记录的loss的回调函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LossHistory</span><span class="params">(Callback)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_train_begin</span><span class="params">(self, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.losses = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_end</span><span class="params">(self, batch, logs=&#123;&#125;)</span>:</span></span><br><span class="line">        self.losses.append(logs.get(<span class="string">'loss'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个自定义的TensorBoard类，专门用来记录batch中的数据变化</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchTensorBoard</span><span class="params">(TensorBoard)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,log_dir=<span class="string">'./logs'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 histogram_freq=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 write_graph=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                 write_images=False)</span>:</span></span><br><span class="line">        super(BatchTensorBoard, self).__init__()</span><br><span class="line">        self.log_dir = log_dir</span><br><span class="line">        self.histogram_freq = histogram_freq</span><br><span class="line">        self.merged = <span class="keyword">None</span></span><br><span class="line">        self.write_graph = write_graph</span><br><span class="line">        self.write_images = write_images</span><br><span class="line">        self.batch = <span class="number">0</span></span><br><span class="line">        self.batch_queue = set()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_batch_end</span><span class="params">(self,batch,logs=None)</span>:</span></span><br><span class="line">        logs = logs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        self.batch = self.batch + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> name, value <span class="keyword">in</span> logs.items():</span><br><span class="line">            <span class="keyword">if</span> name <span class="keyword">in</span> [<span class="string">'batch'</span>, <span class="string">'size'</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            summary = tf.Summary()</span><br><span class="line">            summary_value = summary.value.add()</span><br><span class="line">            summary_value.simple_value = float(value)</span><br><span class="line">            summary_value.tag = <span class="string">"batch_"</span> + name</span><br><span class="line">            <span class="keyword">if</span> (name,self.batch) <span class="keyword">in</span> self.batch_queue:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            self.writer.add_summary(summary, self.batch)</span><br><span class="line">            self.batch_queue.add((name,self.batch))</span><br><span class="line">        self.writer.flush()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensorboard = TensorBoard(log_dir=<span class="string">'/home/tensorflow/log/softmax/epoch'</span>)</span><br><span class="line">my_tensorboard = BatchTensorBoard(log_dir=<span class="string">'/home/tensorflow/log/softmax/batch'</span>)</span><br><span class="line"></span><br><span class="line">model.fit(x_train_1, y_train_1,</span><br><span class="line">          nb_epoch=<span class="number">20</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>,</span><br><span class="line">          callbacks=[tensorboard, my_tensorboard])</span><br></pre></td></tr></table></figure>
<pre><code>&lt;keras.callbacks.History at 0xa86d650&gt;
</code></pre><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><strong>损失函数（loss function）</strong>，是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数，在统计学中损失函数是一种衡量损失和错误（这种损失与“错误地”估计有关，如费用或者设备的损失）程度的函数。</p>
<p><strong>交叉熵（cross-entropy）</strong>就是神经网络中常用的损失函数。</p>
<p>交叉熵性质：</p>
<p>（1）非负性。</p>
<p>（2）当真实输出a与期望输出y接近的时候，代价函数接近于0.(比如y=0，a～0；y=1，a~1时，代价函数都接近0)。</p>
<p><img src="/img/cross-entropy.jpeg"></p>
<p>一个比较简单的理解就是使得 预测值Yi和真实值Y’ 对接近，即两者的乘积越大，coss-entropy越小。</p>
<p>交叉熵和准确度变化图像可以看 <strong>TensorBoard</strong> 。</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>如果对于所有的权重和所有的偏置计算交叉熵的偏导数，就得到一个对于给定图像、标签和当前权重和偏置的「梯度」，如图所示：</p>
<p><img src="/img/scg.jpeg"></p>
<p>我们希望损失函数最小，也就是需要到达交叉熵最小的凹点的低部。在上图中，交叉熵被表示为一个具有两个权重的函数。</p>
<p>而学习速率，即在梯度下降中的步伐大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型的测试误差指标</span></span><br><span class="line">print(model.metrics_names)</span><br><span class="line"><span class="comment"># 对测试数据进行测试</span></span><br><span class="line">model.evaluate(x_test_1, y_test_1,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;loss&apos;, &apos;acc&apos;]
 9800/10000 [============================&gt;.] - ETA: 0s




[0.87580669939517974, 0.94387999653816224]
</code></pre><hr>
<p>上面，我们探索了softmax对多分类的支持和理解，知道softmax可以作为一个输出成层进行多分类任务。</p>
<p>但是，这种分类任务解决的都是线性因素形成的问题，对于非线性的，特别是异或问题，如何解决呢？</p>
<p>这时，一种包含多层隐含层的深度神经网络的概念被提出。</p>
<h3 id="3-激活函数"><a href="#3-激活函数" class="headerlink" title="3. 激活函数"></a>3. 激活函数</h3><p><strong>激活函数（activation function）</strong>可以使得模型加入非线性因素的。</p>
<p>解决非线性问题有两个办法：线性变换、引入非线性函数。</p>
<p><strong>（1）线性变换(linear transformation)</strong></p>
<p>原本一个线性不可分的模型如：X^2 + Y^2 = 1</p>
<p>其图形如下图所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(<span class="number">0</span>)</span><br><span class="line">degree = np.random.rand(<span class="number">50</span>)*np.pi*<span class="number">2</span></span><br><span class="line">x_1 = np.cos(degree)*np.random.rand(<span class="number">50</span>)</span><br><span class="line">y_1 = np.sin(degree)*np.random.rand(<span class="number">50</span>)</span><br><span class="line">x_2 = np.cos(degree)*(<span class="number">1</span>+np.random.rand(<span class="number">50</span>))</span><br><span class="line">y_2 = np.sin(degree)*(<span class="number">1</span>+np.random.rand(<span class="number">50</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_3 和 y_3 就是切分线</span></span><br><span class="line">t = np.linspace(<span class="number">0</span>,np.pi*<span class="number">2</span>,<span class="number">50</span>)</span><br><span class="line">x_3 = np.cos(t)</span><br><span class="line">y_3 = np.sin(t)</span><br><span class="line"></span><br><span class="line">scatter(x_1,y_1,c=<span class="string">'red'</span>,s=<span class="number">50</span>,alpha=<span class="number">0.4</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">scatter(x_2,y_2,c=<span class="string">'black'</span>,s=<span class="number">50</span>,alpha=<span class="number">0.4</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">plot(x_3,y_3)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x86c7510&gt;]
</code></pre><p><img src="/img/LearnOfDeepLearning_12_1.png" alt="png"></p>
<p>将坐标轴进行高维变换，横坐标变成X^2，纵坐标变成 Y^2，这是表达式变为了 X + Y = 1，这样，原来的非线性问题，就变成了一个线性可分的问题，变成了一个简单的一元一次方程了。</p>
<p>详细可以参见下图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fig2 = plt.figure(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#令新的横坐标变成x^2,纵坐标变成 Y^2</span></span><br><span class="line">x_4 = x_1**<span class="number">2</span></span><br><span class="line">y_4 = y_1**<span class="number">2</span></span><br><span class="line">x_5 = x_2**<span class="number">2</span></span><br><span class="line">y_5 = y_2**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这样就可以构建一个一元线性方程进行拟合</span></span><br><span class="line">x_6 = np.linspace(<span class="number">-1</span>,<span class="number">2</span>,<span class="number">50</span>)</span><br><span class="line">y_6 = <span class="number">1</span> - x_6</span><br><span class="line"></span><br><span class="line">scatter(x_4,y_4,c=<span class="string">'red'</span>,s=<span class="number">50</span>,alpha=<span class="number">0.4</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">scatter(x_5,y_5,c=<span class="string">'black'</span>,s=<span class="number">50</span>,alpha=<span class="number">0.4</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">plot(x_6,y_6)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x984bf10&gt;]
</code></pre><p><img src="/img/LearnOfDeepLearning_14_1.png" alt="png"></p>
<p><strong>（2）引入非线性函数</strong></p>
<p>异或是一种基于二进制的位运算，用符号XOR 表示(Python中的异或操作符为 <strong>^</strong> )，其运算法则是对运算符两侧数的每一个二进制位，同值取0，异值取1。</p>
<p>下面是一个典型的异或表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">table = &#123;<span class="string">'x'</span>:[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],<span class="string">'y'</span>:[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(table)</span><br><span class="line">df[<span class="string">'z'</span>] = df[<span class="string">'x'</span>]^df[<span class="string">'y'</span>]</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<div><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th></th>      <th>x</th>      <th>y</th>      <th>z</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <th>3</th>      <td>0</td>      <td>1</td>      <td>1</td>    </tr>  </tbody></table></div>

<p>x = 1, y = 1, 则 z = 0</p>
<p>x = 0, y = 0, 则 z = 0</p>
<p>x = 1, y = 0, 则 z = 1</p>
<p>x = 0, y = 1, 则 z = 1</p>
<p>…</p>
<p>其图形如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig3 = plt.figure(<span class="number">2</span>)</span><br><span class="line">groups = df.groupby(<span class="string">'z'</span>)</span><br><span class="line"><span class="keyword">for</span> name, group <span class="keyword">in</span> groups:</span><br><span class="line">    scatter(group[<span class="string">'x'</span>],group[<span class="string">'y'</span>],label=name,s=<span class="number">50</span>,marker=<span class="string">'o'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/img/LearnOfDeepLearning_18_0.png" alt="png"></p>
<p>那么如果可以构建一个函数拟合这样的图形呢？即如何构建一个f()，使得：f(x,y)=z呢？</p>
<hr>
<p>为了解决问题，我们来构建一个两层的神经网络，该神经网络有两个激活函数，F(x,y)和 H(x,y), 具体如下图所示：</p>
<p><img src="/img/ORX_problem.png"></p>
<p>F(x,y)为一个阈值为1的阈值函数：</p>
<p>即：当AX+BY&gt;1时候,F(x,y) = 1;否则为0；<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if AX+BY &gt; 1:</span><br><span class="line">    F = 1</span><br><span class="line">else:</span><br><span class="line">    F = 0</span><br></pre></td></tr></table></figure></p>
<p>H(x,y）为一个阈值为0的阈值函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if AX+BY &gt; 0:</span><br><span class="line">    H = 1</span><br><span class="line">else:</span><br><span class="line">    H = 0</span><br></pre></td></tr></table></figure></p>
<p>图中线的数字表示权重值，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- 对于(1,1)的点，第二层从左到右隐藏层的值分别为(1,1,1),最后输出为(1,1,1)*(1,-2,1)=0；</span><br><span class="line"></span><br><span class="line">- 对于(0,0)的点，第二层从左到右隐藏层的值分别为(0,0,0),最后输出为(0,0,0)*(1,-2,1)=0；</span><br><span class="line"></span><br><span class="line">- 对于(1,0)的点，第二层从左到右隐藏层的值分别为(1,0,0),最后输出为(1,0,0)*(1,-2,1)= 1；</span><br><span class="line"></span><br><span class="line">- 对于(0,1)的点，第二层从左到右隐藏层的值分别为(0,0,1),最后输出为(0,0,1)*(1,-2,1)= 1；</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">first_hidder_layer_table = &#123;<span class="string">'x'</span>:[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],<span class="string">'y'</span>:[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],<span class="string">'z'</span>:[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],<span class="string">'output'</span>:[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]&#125;</span><br><span class="line">first_hidder_layer_data = pd.DataFrame(first_hidder_layer_table)</span><br><span class="line">first_hidder_layer_data</span><br></pre></td></tr></table></figure>
<div><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>output</th>      <th>x</th>      <th>y</th>      <th>z</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>1</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>1</th>      <td>0</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>0</td>      <td>0</td>      <td>1</td>    </tr>  </tbody></table></div>

<hr>
<p>这样我们就构建出了一个可以计算拟合的函数了。</p>
<p>我们观察一下第一个隐含层，其总共有三个维度，三个权重值，从输入层到第一层，实际上，就是从将一个二维的数组变成一个三维数组，从而实现线性切分。</p>
<p>图形化解释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">fig4 = plt.figure(<span class="number">3</span>)</span><br><span class="line">ax = fig4.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">groups = first_hidder_layer_data.groupby(<span class="string">'output'</span>)</span><br><span class="line"><span class="keyword">for</span> name, group <span class="keyword">in</span> groups:</span><br><span class="line">    ax.scatter(group[<span class="string">'x'</span>],group[<span class="string">'y'</span>],group[<span class="string">'z'</span>],label=name,c=np.random.choice([<span class="string">'black'</span>,<span class="string">'blue'</span>]),s=<span class="number">50</span>,marker=<span class="string">'o'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'X Label'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Y Label'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'Z Label'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.text.Text at 0xb7b0d90&gt;
</code></pre><p><img src="/img/LearnOfDeepLearning_22_1.png" alt="png"></p>
<p>经过变换后的数据是线性可分的（n维，比如本例中可以用平面将两个不同颜色的点切分）</p>
<p>更多的操作可以参考tensorflow提供的一个神经网络的网页小程序，通过自己调整程序参数可以更深刻理解神经网络、激活函数的作用。</p>
<p>演示网址：</p>
<p><a href="http://playground.tensorflow.org/" target="_blank" rel="noopener">http://playground.tensorflow.org/</a></p>
<p>可以自己建立一个小型神经网络帮助理解。</p>
<h3 id="4-sigmoid"><a href="#4-sigmoid" class="headerlink" title="4. sigmoid"></a>4. sigmoid</h3><p>sigmoid是一个用来做二分类的”S”形逻辑回归曲线</p>
<p>sigmoid公式：<br><img src="/img/sigmoid-formula.jpg"></p>
<p>sigmoid图像：<br><img src="/img/sigmoid-img.jpg" style="weight=50%"></p>
<p>其抑制两头,对中间细微变化敏感，因此sigmoid函数作为最简单常用的神经网络激活层被使用。</p>
<p>优点：</p>
<p>（1）输出范围(0,1)，数据在传递的过程中不容易发散</p>
<p>（2）单向递增</p>
<p>（3）易求导</p>
<p>sigmod有个缺点，sigmoid函数反向传播时，很容易就会出现梯度消失,在接近饱和区的时候，导数趋向0，会变得非常缓慢。因此，在优化器选择时选用Adam优化器。</p>
<p>Adam 也是基于梯度下降的方法，但是每次迭代参数的学习步长都有一个确定的范围，不会因为很大的梯度导致很大的学习步长，参数的值比较稳定。有利于降低模型收敛到局部最优的风险，而SGD容易收敛到局部最优，<strong>如果下面代码中的optimizer改成SGD的化，在一次epoch后就acc值不会改变了，陷入局部最优</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 构建一个五层sigmod全连接神经网络</span></span><br><span class="line"><span class="comment"># neural network with 5 layers</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># · · · · · · · · · ·       (input data, flattened pixels)       X [batch, 784]   # 784 = 28*28</span></span><br><span class="line"><span class="comment"># \x/x\x/x\x/x\x/x\x/    -- fully connected layer (sigmoid)      W1 [784, 200]      B1[200]</span></span><br><span class="line"><span class="comment">#  · · · · · · · · ·                                             Y1 [batch, 200]</span></span><br><span class="line"><span class="comment">#   \x/x\x/x\x/x\x/      -- fully connected layer (sigmoid)      W2 [200, 100]      B2[100]</span></span><br><span class="line"><span class="comment">#    · · · · · · ·                                               Y2 [batch, 100]</span></span><br><span class="line"><span class="comment">#    \x/x\x/x\x/         -- fully connected layer (sigmoid)      W3 [100, 60]       B3[60]</span></span><br><span class="line"><span class="comment">#     · · · · ·                                                  Y3 [batch, 60]</span></span><br><span class="line"><span class="comment">#     \x/x\x/            -- fully connected layer (sigmoid)      W4 [60, 30]        B4[30]</span></span><br><span class="line"><span class="comment">#      · · ·                                                     Y4 [batch, 30]</span></span><br><span class="line"><span class="comment">#      \x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]</span></span><br><span class="line"><span class="comment">#       ·                                                        Y5 [batch, 10]</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">200</span>, input_shape=(<span class="number">784</span>,)))<span class="comment">#全连接，输入784维度, 输出10维度，需要和输入输出对应</span></span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line">model.add(Dense(<span class="number">100</span>))<span class="comment"># 除了首层需要设置输入维度，其他层只需要输入输出维度就可以了，输入维度自动继承上层。</span></span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line">model.add(Dense(<span class="number">60</span>))</span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))</span><br><span class="line">model.add(Dense(<span class="number">30</span>))            <span class="comment">#model.add(Activation('sigmoid'))和model.add(Dense(30))可以合并写出</span></span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))<span class="comment">#model.add(Dense(30,activation='softmax'))</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">sgd = Adam(lr=<span class="number">0.003</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              optimizer=sgd,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#model 概要</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dense_23 (Dense)                 (None, 200)           157000      dense_input_7[0][0]              
____________________________________________________________________________________________________
activation_23 (Activation)       (None, 200)           0           dense_23[0][0]                   
____________________________________________________________________________________________________
dense_24 (Dense)                 (None, 100)           20100       activation_23[0][0]              
____________________________________________________________________________________________________
activation_24 (Activation)       (None, 100)           0           dense_24[0][0]                   
____________________________________________________________________________________________________
dense_25 (Dense)                 (None, 60)            6060        activation_24[0][0]              
____________________________________________________________________________________________________
activation_25 (Activation)       (None, 60)            0           dense_25[0][0]                   
____________________________________________________________________________________________________
dense_26 (Dense)                 (None, 30)            1830        activation_25[0][0]              
____________________________________________________________________________________________________
activation_26 (Activation)       (None, 30)            0           dense_26[0][0]                   
____________________________________________________________________________________________________
dense_27 (Dense)                 (None, 10)            310         activation_26[0][0]              
____________________________________________________________________________________________________
activation_27 (Activation)       (None, 10)            0           dense_27[0][0]                   
====================================================================================================
Total params: 185,300
Trainable params: 185,300
Non-trainable params: 0
____________________________________________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensorboard2 = TensorBoard(log_dir=<span class="string">'/home/tensorflow/log/five_layer_sigmoid/epoch'</span>, histogram_freq=<span class="number">0</span>)</span><br><span class="line">my_tensorboard2 = BatchTensorBoard(log_dir=<span class="string">'/home/tensorflow/log/five_layer_sigmoid/batch'</span>)</span><br><span class="line">model.fit(x_train_1, y_train_1,</span><br><span class="line">          nb_epoch=<span class="number">20</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>,</span><br><span class="line">          callbacks=[my_tensorboard2, tensorboard2])</span><br></pre></td></tr></table></figure>
<pre><code>&lt;keras.callbacks.History at 0xf868a90&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型的测试误差指标</span></span><br><span class="line">print(model.metrics_names)</span><br><span class="line"><span class="comment"># 对测试数据进行测试</span></span><br><span class="line">model.evaluate(x_test_1, y_test_1,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;loss&apos;, &apos;acc&apos;]
 9800/10000 [============================&gt;.] - ETA: 0s

[0.036339853547979147, 0.98736999988555907]
</code></pre><hr>
<p>根据上面，我们可以看出，深度越深，效果越好。</p>
<p>但是，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况从而无法完成深层网络的训练。在sigmoid接近饱和区时，变换非常缓慢，导数趋于0，减缓收敛速度。</p>
<h3 id="5-ReLu"><a href="#5-ReLu" class="headerlink" title="5. ReLu"></a>5. ReLu</h3><p>ReLu来自于对人脑神经细胞工作时的稀疏性的研究，在 Lennie,P.(2003)提出人脑神经元有95%－99%是闲置的，而更少工作的神经元意味着更小的计算复杂度，更不容易过拟合</p>
<p>修正线性单元(Rectified linear unit,ReLU)公式：</p>
<p>$$ReLU\left(x\right)=<br>\left\lbrace<br>\begin{align}<br>x, \quad x \gt 0 \cr<br>0, \quad x \le 0<br>\end{align}<br>\right .$$</p>
<p>其图像：</p>
<p><img src="/img/relu-img.jpg"></p>
<p>ReLU具有线性、非饱和性，而其非饱和性使得网络可以自行引入稀疏性。</p>
<p>ReLU的使用解决了sigmoid梯度下降慢，深层网络的信息丢失的问题。</p>
<p>ReLU在训练时是非常脆弱的，并且可能会“死”。例如，经过ReLU神经元的一个大梯度可能导致权重更新后该神经元接收到任何数据点都不会再激活。如果发生这种情况，之后通过该单位点的梯度将永远是零。也就是说，ReLU可能会在训练过程中不可逆地死亡，并且破坏数据流形。如果学习率太高，大部分网络将会“死亡”（即，在整个训练过程中神经元都没有激活）。而设置一个适当的学习率，可以在一定程度上避免这一问题。</p>
<h3 id="6-学习速率"><a href="#6-学习速率" class="headerlink" title="6. 学习速率"></a>6. 学习速率</h3><p>上面说梯度下降的时候，说过学习速率其实就是梯度下降的步伐。因此，为了到达山谷，需要控制步伐的大小，即学习速率。</p>
<p>学习速率大小的调节一般取决于 loss 的变化幅度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># neural network with 5 layers</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># · · · · · · · · · ·       (input data, flattened pixels)       X [batch, 784]   # 784 = 28*28</span></span><br><span class="line"><span class="comment"># \x/x\x/x\x/x\x/x\x/    -- fully connected layer (relu)         W1 [784, 200]      B1[200]</span></span><br><span class="line"><span class="comment">#  · · · · · · · · ·                                             Y1 [batch, 200]</span></span><br><span class="line"><span class="comment">#   \x/x\x/x\x/x\x/      -- fully connected layer (relu)         W2 [200, 100]      B2[100]</span></span><br><span class="line"><span class="comment">#    · · · · · · ·                                               Y2 [batch, 100]</span></span><br><span class="line"><span class="comment">#    \x/x\x/x\x/         -- fully connected layer (relu)         W3 [100, 60]       B3[60]</span></span><br><span class="line"><span class="comment">#     · · · · ·                                                  Y3 [batch, 60]</span></span><br><span class="line"><span class="comment">#     \x/x\x/            -- fully connected layer (relu)         W4 [60, 30]        B4[30]</span></span><br><span class="line"><span class="comment">#      · · ·                                                     Y4 [batch, 30]</span></span><br><span class="line"><span class="comment">#      \x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]</span></span><br><span class="line"><span class="comment">#       ·                                                        Y5 [batch, 10]</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">200</span>, input_shape=(<span class="number">784</span>,)))<span class="comment">#全连接，输入784维度, 输出10维度，需要和输入输出对应</span></span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))<span class="comment"># 将激活函数sigmoid改为ReLU</span></span><br><span class="line">model.add(Dense(<span class="number">100</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">60</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">30</span>))            </span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">sgd = Adam(lr=<span class="number">0.001</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              optimizer=sgd,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#model 概要</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dense_16 (Dense)                 (None, 200)           157000      dense_input_4[0][0]              
____________________________________________________________________________________________________
activation_16 (Activation)       (None, 200)           0           dense_16[0][0]                   
____________________________________________________________________________________________________
dense_17 (Dense)                 (None, 100)           20100       activation_16[0][0]              
____________________________________________________________________________________________________
activation_17 (Activation)       (None, 100)           0           dense_17[0][0]                   
____________________________________________________________________________________________________
dense_18 (Dense)                 (None, 60)            6060        activation_17[0][0]              
____________________________________________________________________________________________________
activation_18 (Activation)       (None, 60)            0           dense_18[0][0]                   
____________________________________________________________________________________________________
dense_19 (Dense)                 (None, 30)            1830        activation_18[0][0]              
____________________________________________________________________________________________________
activation_19 (Activation)       (None, 30)            0           dense_19[0][0]                   
____________________________________________________________________________________________________
dense_20 (Dense)                 (None, 10)            310         activation_19[0][0]              
____________________________________________________________________________________________________
activation_20 (Activation)       (None, 10)            0           dense_20[0][0]                   
====================================================================================================
Total params: 185,300
Trainable params: 185,300
Non-trainable params: 0
____________________________________________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensorboard3 = TensorBoard(log_dir=<span class="string">'/home/tensorflow/log/five_layer_relu/epoch'</span>, histogram_freq=<span class="number">0</span>)</span><br><span class="line">my_tensorboard3 = BatchTensorBoard(log_dir=<span class="string">'/home/tensorflow/log/five_layer_relu/batch'</span>)</span><br><span class="line">model.fit(x_train_1, y_train_1,</span><br><span class="line">          nb_epoch=<span class="number">30</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>,</span><br><span class="line">          callbacks=[my_tensorboard3, tensorboard3])</span><br></pre></td></tr></table></figure>
<pre><code>&lt;keras.callbacks.History at 0xe3c6d50&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型的测试误差指标</span></span><br><span class="line">print(model.metrics_names)</span><br><span class="line"><span class="comment"># 对测试数据进行测试</span></span><br><span class="line">model.evaluate(x_test_1, y_test_1,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;loss&apos;, &apos;acc&apos;]
 9600/10000 [===========================&gt;..] - ETA: 0s




[0.017244604945910281, 0.99598000288009647]
</code></pre><h3 id="7-Dropout"><a href="#7-Dropout" class="headerlink" title="7.Dropout"></a>7.Dropout</h3><hr>
<p>运行目录下的<a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.1_five_layers_relu_lrdecay.py" target="_blank" rel="noopener">mnist_2.1_five_layers_relu_lrdecay.py</a></p>
<p><img src="/img/big-gap.png" style="width: 50%"></p>
<p>随着迭代次数的增加，我们可以发现测试数据的loss值和训练数据的loss存在着巨大的差距， 随着迭代次数增加，train loss 越来越好，但test loss 的结果确越来越差，test loss 和 train loss 差距越来越大，模型开始过拟合。</p>
<p>Dropout是指对于神经网络单元按照一定的概率将其暂时从网络中丢弃,从而解决过拟合问题。</p>
<p><img src="/img/dropout-1.png" style="width: 50%"></p>
<p>可以对比<a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.1_five_layers_relu_lrdecay.py" target="_blank" rel="noopener">mnist_2.1_five_layers_relu_lrdecay.py</a> 和 加了dropout的<a href="https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.2_five_layers_relu_lrdecay_dropout.py" target="_blank" rel="noopener">/mnist_2.2_five_layers_relu_lrdecay_dropout.py</a>的结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># neural network with 5 layers</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># · · · · · · · · · ·       (input data, flattened pixels)       X [batch, 784]   # 784 = 28*28</span></span><br><span class="line"><span class="comment"># \x/x\x/x\x/x\x/x\x/ ✞  -- fully connected layer (relu+dropout) W1 [784, 200]      B1[200]</span></span><br><span class="line"><span class="comment">#  · · · · · · · · ·                                             Y1 [batch, 200]</span></span><br><span class="line"><span class="comment">#   \x/x\x/x\x/x\x/ ✞    -- fully connected layer (relu+dropout) W2 [200, 100]      B2[100]</span></span><br><span class="line"><span class="comment">#    · · · · · · ·                                               Y2 [batch, 100]</span></span><br><span class="line"><span class="comment">#    \x/x\x/x\x/ ✞       -- fully connected layer (relu+dropout) W3 [100, 60]       B3[60]</span></span><br><span class="line"><span class="comment">#     · · · · ·                                                  Y3 [batch, 60]</span></span><br><span class="line"><span class="comment">#     \x/x\x/ ✞          -- fully connected layer (relu+dropout) W4 [60, 30]        B4[30]</span></span><br><span class="line"><span class="comment">#      · · ·                                                     Y4 [batch, 30]</span></span><br><span class="line"><span class="comment">#      \x/               -- fully connected layer (softmax)      W5 [30, 10]        B5[10]</span></span><br><span class="line"><span class="comment">#       ·                                                        Y5 [batch, 10]</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">200</span>, input_shape=(<span class="number">784</span>,)))<span class="comment">#全连接，输入784维度, 输出10维度，需要和输入输出对应</span></span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))<span class="comment"># 将激活函数sigmoid改为ReLU</span></span><br><span class="line">model.add(Dense(<span class="number">100</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))<span class="comment"># 添加一个dropout层, 随机移除25%的单元</span></span><br><span class="line">model.add(Dense(<span class="number">60</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line">model.add(Dense(<span class="number">30</span>))            </span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line">model.add(Dense(<span class="number">10</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">sgd = Adam(lr=<span class="number">0.001</span>)</span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              optimizer=sgd,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#model 概要</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
dense_171 (Dense)                (None, 200)           157000      dense_input_35[0][0]             
____________________________________________________________________________________________________
activation_171 (Activation)      (None, 200)           0           dense_171[0][0]                  
____________________________________________________________________________________________________
dense_172 (Dense)                (None, 100)           20100       activation_171[0][0]             
____________________________________________________________________________________________________
activation_172 (Activation)      (None, 100)           0           dense_172[0][0]                  
____________________________________________________________________________________________________
dropout_100 (Dropout)            (None, 100)           0           activation_172[0][0]             
____________________________________________________________________________________________________
dense_173 (Dense)                (None, 60)            6060        dropout_100[0][0]                
____________________________________________________________________________________________________
activation_173 (Activation)      (None, 60)            0           dense_173[0][0]                  
____________________________________________________________________________________________________
dropout_101 (Dropout)            (None, 60)            0           activation_173[0][0]             
____________________________________________________________________________________________________
dense_174 (Dense)                (None, 30)            1830        dropout_101[0][0]                
____________________________________________________________________________________________________
activation_174 (Activation)      (None, 30)            0           dense_174[0][0]                  
____________________________________________________________________________________________________
dropout_102 (Dropout)            (None, 30)            0           activation_174[0][0]             
____________________________________________________________________________________________________
dense_175 (Dense)                (None, 10)            310         dropout_102[0][0]                
____________________________________________________________________________________________________
activation_175 (Activation)      (None, 10)            0           dense_175[0][0]                  
====================================================================================================
Total params: 185,300
Trainable params: 185,300
Non-trainable params: 0
____________________________________________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensorboard4 = TensorBoard(log_dir=<span class="string">'/home/tensorflow/log/five_layer_relu_dropout/epoch'</span>)</span><br><span class="line">my_tensorboard4 = BatchTensorBoard(log_dir=<span class="string">'/home/tensorflow/log/five_layer_relu_dropout/batch'</span>)</span><br><span class="line"></span><br><span class="line">model.fit(x_train_1, y_train_1,</span><br><span class="line">          nb_epoch=<span class="number">30</span>,</span><br><span class="line">          verbose=<span class="number">0</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>,</span><br><span class="line">          callbacks=[tensorboard4, my_tensorboard4])</span><br></pre></td></tr></table></figure>
<pre><code>&lt;keras.callbacks.History at 0x27819610&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型的测试误差指标</span></span><br><span class="line">print(model.metrics_names)</span><br><span class="line"><span class="comment"># 对测试数据进行测试</span></span><br><span class="line">model.evaluate(x_test_1, y_test_1,</span><br><span class="line">          verbose=<span class="number">1</span>,</span><br><span class="line">          batch_size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;loss&apos;, &apos;acc&apos;]
 9900/10000 [============================&gt;.] - ETA: 0s




[0.025450729207368569, 0.99462999999523161]
</code></pre>
      
    </div>

    <div>
      
        
<div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/img/wechat-qcode.jpg" alt="shikanon wechat" style="width: 200px; max-width: 100%;">
    <div>欢迎您扫一扫，订阅上面↑↑↑的微信公众号！</div>
</div>


      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/Keras/" rel="tag"># Keras</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/06/20/fabric-初步实践/" rel="next" title="fabric 初步实践">
                <i class="fa fa-chevron-left"></i> fabric 初步实践
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/02/15/Hexo使用笔记/" rel="prev" title="Hexo使用文档">
                Hexo使用文档 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        
<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/02/14/LearnOfDeepLearning/" data-title="深度学习从小白到入门 —— 基于keras的深度学习基本概念讲解" data-url="http://yoursite.com/2017/02/14/LearnOfDeepLearning/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="shikanon">
          <p class="site-author-name" itemprop="name">shikanon</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/shikanon" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://my.oschina.net/Kanonpy" target="_blank" title="oschina">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  oschina
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/shikanon" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-tencent-weibo"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习"><span class="nav-number">1.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Author-shikanon"><span class="nav-number">1.0.1.</span> <span class="nav-text">Author: shikanon</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CreateTime-2017-2-13"><span class="nav-number">1.0.2.</span> <span class="nav-text">CreateTime: 2017/2/13</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一、基础篇"><span class="nav-number">1.1.</span> <span class="nav-text">一、基础篇</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-softmax"><span class="nav-number">1.1.1.</span> <span class="nav-text">1. softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-number">1.1.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降"><span class="nav-number">1.1.3.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-激活函数"><span class="nav-number">1.1.4.</span> <span class="nav-text">3. 激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-sigmoid"><span class="nav-number">1.1.5.</span> <span class="nav-text">4. sigmoid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-ReLu"><span class="nav-number">1.1.6.</span> <span class="nav-text">5. ReLu</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-学习速率"><span class="nav-number">1.1.7.</span> <span class="nav-text">6. 学习速率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Dropout"><span class="nav-number">1.1.8.</span> <span class="nav-text">7.Dropout</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">shikanon</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"shikanon.duoshuo.com"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


</body>
</html>
